{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "520myve5p4cU",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1db63d1a-f37a-49f2-8011-6b9d0946c431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pdfplumber langchain langchain-community langchain_huggingface chromadb huggingface_hub sentence-transformers nltk torch gradio transformers accelerate gradio_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ivD66efqNoO",
    "outputId": "0ac779eb-dfa8-4fd4-bec9-ec8fc98c610a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import torch\n",
    "import gradio as gr\n",
    "from gradio_pdf import PDF\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRtGYhbAtL5U"
   },
   "source": [
    "<h2>Preprocessing the PDF</h2>\n",
    "<h4>Extracting the text using pdfplumber and cleaning it using regex.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SmUQWnqVqoG0"
   },
   "outputs": [],
   "source": [
    "def read_and_clean_pdf(PDF):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(PDF) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                page_text = re.sub(r'\\n+', '\\n', page_text)\n",
    "                page_text = re.sub(r'[ \\t]+', ' ', page_text)\n",
    "                page_text = re.sub(r'[^\\x00-\\x7F]+', ' ', page_text)\n",
    "                page_text = re.sub(r'-\\n', '', page_text)\n",
    "                text += page_text + \"\\n\"\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "KUhDeS5hq-Au",
    "outputId": "f9a0a5c5-15d3-4967-d728-4cb01cc8dfe8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-6f3b4f8a-05e0-463a-9478-5399e4a0aefa\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-6f3b4f8a-05e0-463a-9478-5399e4a0aefa\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ugrulebook.pdf to ugrulebook (1).pdf\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "606nB2Hwq_jW"
   },
   "outputs": [],
   "source": [
    "sample=read_and_clean_pdf('ugrulebook.pdf')    # \"sample\" is the testing pdf for testing on the go. Will later build the UI for any pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5hGBH9ItL5X"
   },
   "source": [
    "<h2>Chunking the extracted text.</h2>\n",
    "<h4>The text need to be broken into chunks for the chatbot to find context of the user query. Using NLTKTectSplitter for better context retention in each chunk without breaking mid-sentence.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "O8vDmxGWrD6n"
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1500, overlap=100):\n",
    "    splitter = NLTKTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "cmjOcgvxrFWK"
   },
   "outputs": [],
   "source": [
    "sample_chunks=chunk_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzi8R2JotL5Y",
    "outputId": "63d01bfe-f6be-4ea7-aaaa-49970929070b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pdf was broken into 92 chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The pdf was broken into {len(sample_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ_62YJ_tL5Y"
   },
   "source": [
    "<h2>Embedding the chunks into vectors and storing them in a database.</h2>\n",
    "<h4>1. Used 'multi-qa-MiniLM-L6-cos-v1' for embedding wrapped in HuggingFaceEmbedding for easier compatibility with Langchain. </h4>\n",
    "<h4>2. Used ChromaDB from Langchain's vectorstore for building the vector database.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0zEekX9srHZu"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"put your hf token here\")         # The hf token has been removed from the original code for privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "fwo7tDdJrIeW"
   },
   "outputs": [],
   "source": [
    "def embed_and_store_chunks(chunks):\n",
    "\n",
    "    embedding_model = HuggingFaceEndpointEmbeddings(\n",
    "        model=\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\",\n",
    "    )\n",
    "\n",
    "    vector_database = Chroma.from_texts(\n",
    "        chunks,\n",
    "        embedding=embedding_model,\n",
    "    )\n",
    "\n",
    "    return vector_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "oTBlKVVTrJg8"
   },
   "outputs": [],
   "source": [
    "sample_database = embed_and_store_chunks(sample_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "IYxls1n5tL5Z"
   },
   "outputs": [],
   "source": [
    "#print(sample_chunks)     #can run to see the chunks for our sample pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f0-DbETtL5Z"
   },
   "source": [
    "<h2>Paraphrasing the user query.</h2>\n",
    "<h4>Using the 'humarin/chatgpt_paraphraser_on_T5_base model' to paraphrase the user query into 2 more queries for better context retrieval. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258,
     "referenced_widgets": [
      "28602c0bdb574062af82258443164fe4",
      "bf24caf841b54a2ca7011ac053774218",
      "5ce23bff941e4355983d344791760aaa",
      "f652de434a224d80836a70d74a9e1ce4",
      "dab599e218564c06b02e96b1a35b3ca5",
      "ad26df426b5a4262b267f230c93d6318",
      "39e2dba21708418dabf55c380868d990",
      "9995f896667e4f67aee476b871196fee",
      "c0d9a012fe8d45789b66852590003fb0",
      "df13f634b22047af92cb651dca7f409b",
      "e8102755a65d43b68d127c3605a834f4",
      "3500f5b5baf34cd0b64bff8b42cff7e0",
      "68fa7e32a65f4f6d8acd01a7348fb9db",
      "4f84a5a8700f40ffa5dfbd2174e07ddf",
      "7ea4be88df694bc88d7704a9ed66ec02",
      "7731bb7decab4d54b63d25c617b9ea6a",
      "b33a93484da84c0391051dcd814be59d",
      "40138d1010d14335962b1660d03a2048",
      "a665ae6752234fa9a30f892414535f65",
      "d7d1963dc1484bc3b5344ea7e15643f6",
      "0a099c763f88400ab832233eace8a7d6",
      "1a8d9edbc7774e22b659102dbc2aa30e",
      "06e027994c0b49c3890b13f70b9b6e7e",
      "651ad5f31a7e4c82985d9f5be1dab22b",
      "ff6250aa7f67422fbec480855554bc0d",
      "655ddea7ae2b4be9850b552391aa4e25",
      "e63b5950a47f4ae8a9ecaed49de4f992",
      "214fff319b3241cdbc4433a0c3474513",
      "5e84750f195f4879b33a7b2b75f5dfc8",
      "ccf52ac4a7144436af3ce9ddae2593da",
      "8bb8c9c0503946b9874f1a5c6f3fe5a7",
      "4d554584701c44d1bfb8a17c89de37f5",
      "590f1f14bfc84aa49e2990bcf3a688f9",
      "f02b6378e6ea438c9a9f84516b111802",
      "fd05b691d7574aa191b86237d86fc3ae",
      "5637a3314975498193787a7bff4740d7",
      "c68d3e261bc74d0aaa20abb64554bd1d",
      "cf444e16f0394cc5a55ab3495421146f",
      "2fc0de5c6c2942538b6335c4c6b67e7f",
      "18d0335b68c446df9d5b28eea94f91a7",
      "5283835062714dcd99da7965d69f9525",
      "3564ace7d4d14a449583eaf89a6f9deb",
      "e67412debcf84e05bd797a3fbe144ade",
      "dde9f99706904bc6875736273efd5962",
      "94fdbe3d43134a67b268269ed42a8741",
      "0e377a23b80743ec8ec7fc325ce0bb4a",
      "b7f7ecf2ca10469bae090780da4dfbc7",
      "1a05f17db8f74a919492900fcadc6adf",
      "4714f46df7a64c71b0ed5e50479dc051",
      "946c026c51b848c2a06c401223a466bb",
      "37244058bada43a6917b57c73e9da231",
      "a1f86ef7f1754a499729a603fe8120ce",
      "45c5d010acba4975bc851a5d852856da",
      "c0962931df0744928a3d8f7cd5bcce11",
      "0fd7d65e378f4fdf92d01b125ad86341",
      "06ba7cd138d54cad94f46fffa1b1f5c9",
      "4041e241e4bf46699f39d02e9c7ded8d",
      "d2dc9dc53e4f460b88c619136de49f47",
      "2f7f0f952b204e6586806d0d835c726f",
      "f9c622180233483c8a64ee3be4fe07dc",
      "9962aecfe0d34f8ea418bee906ce2ca2",
      "9700a8a8496a4b6ab5eebd4df1267038",
      "153b219ee37746b59e64b369b4c1715b",
      "b701972acb794f70a68598f42cb00333",
      "8696b8afede04fab9274c211b58c3336",
      "eeb29aba67d346e0ae6c4546970668c3",
      "f0ad055588b84f138132611e10e34218",
      "a1355949d8ff46b9929e0d97b54564d0",
      "06c5d1b946be429287030e824021e54e",
      "01d35eb2fc3143d6a1e8a2000b9e66e9",
      "b001d7a426f34795b560213a9f467efd",
      "d526ce52907c4ee7baa2c4466e8c8e64",
      "2e5ff08761f440abab704ca3020c2c23",
      "c8345a388a0e4477bbe1d3db8b5a31cc",
      "308fda7823d94821a08be641992c1bbc",
      "ad2d6e96038c49ea807ac200156d8fe4",
      "575f49f9a3364816a86429715c63307a"
     ]
    },
    "id": "tjzCllBYrK7B",
    "outputId": "888d40fb-9c5a-425c-ed10-ac5166fb2e29"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28602c0bdb574062af82258443164fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3500f5b5baf34cd0b64bff8b42cff7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e027994c0b49c3890b13f70b9b6e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02b6378e6ea438c9a9f84516b111802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fdbe3d43134a67b268269ed42a8741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ba7cd138d54cad94f46fffa1b1f5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ad055588b84f138132611e10e34218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrasing model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "paraphrasing_model_name = \"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    "paraphrasing_tokenizer = AutoTokenizer.from_pretrained(paraphrasing_model_name)\n",
    "paraphrasing_model = AutoModelForSeq2SeqLM.from_pretrained(paraphrasing_model_name)\n",
    "\n",
    "print(\"Paraphrasing model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "fFZoM-UIrN1V"
   },
   "outputs": [],
   "source": [
    "def generate_paraphrases(text, num_return_sequences=2):\n",
    "    prompt = f\"paraphrase : {text} </s>\"\n",
    "    encoding = paraphrasing_tokenizer.encode_plus(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        paraphrasing_model.to(\"cuda\")\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "        attention_mask = attention_mask.to(\"cuda\")\n",
    "\n",
    "    outputs = paraphrasing_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=300,\n",
    "        do_sample=False,\n",
    "        num_beam_groups=3,\n",
    "        diversity_penalty=2.0,\n",
    "        num_beams=6,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=1,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    paraphrases = [\n",
    "        paraphrasing_tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for output in outputs\n",
    "    ]\n",
    "\n",
    "    return paraphrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6NO-EN-rPF2",
    "outputId": "2c35ec76-5931-408d-b26d-5a3d1bee1fd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the protocol for substituting courses?',\n",
       " 'What is the rule for substituting other courses?']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_paraphrases('What is the policy for course substitution?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeYapoWytL5a"
   },
   "source": [
    "<h2>Retrieving relevant chunks</h2>\n",
    "<h4>Now based on the user query and the paraphrasing model-generated queries, the best 3 chunks are selected for each query and then top 3 of all the selected chunks are selected.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "m7ZuOXLtrRJY"
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, vector_database, paraphrasing_function, max_chunks=3):\n",
    "    queries = [query]\n",
    "    queries.extend(paraphrasing_function(query))\n",
    "\n",
    "    chunks = []\n",
    "    for query in queries:\n",
    "        results = vector_database.similarity_search_with_score(query, k=3)\n",
    "        for chunk,score in results:\n",
    "\n",
    "          chunks.append((chunk.page_content, score))\n",
    "\n",
    "    chunk_score_dict = {}\n",
    "    for content, score in chunks:\n",
    "        if content not in chunk_score_dict or score > chunk_score_dict[content]:\n",
    "            chunk_score_dict[content] = score\n",
    "\n",
    "    sorted_chunks = sorted(chunk_score_dict.items(), key=lambda x: x[1])\n",
    "    unique_chunks = [chunk for chunk, _ in sorted_chunks[:max_chunks]]\n",
    "\n",
    "    return unique_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOYvfNwPtL5b"
   },
   "source": [
    "<h2>Building the prompt generation function.</h2>\n",
    "<h4>This function takes the user query and the retrieved chunks and builds them into a prompt for the LLM behind our chatbot.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "QHxGi3izrTYR"
   },
   "outputs": [],
   "source": [
    "def prompt(query, relevant_chunks):\n",
    "    system_prompt = \"\"\"\n",
    "\n",
    "You are an AI assistant answering user questions based solely on the provided context.\n",
    "\n",
    "The question will be prefixed with \"Question:\"\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- Use only the context given (marked as \"Context\").\n",
    "- Format clearly using proper language and structure and newlines.\n",
    "- You must strictly use only the provided context.\n",
    "- Do not invent or assume any information.\n",
    "- Be very descriptive so that you utilize all the relevant info given.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "\n",
    "    return f\"\"\"{system_prompt}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP7jdH-ZtL5b"
   },
   "source": [
    "<h2>Initiating the LLM</h2>\n",
    "<h4>Using \"meta-llama/Llama-2-7b-chat-hf\" from Hugging Face.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404,
     "referenced_widgets": [
      "841c1311940c4dbdabcf25323b69aff7",
      "d66d9daa7791429b947ec995f4e8316a",
      "91b254dbabfe4df7af81f0b556ba5410",
      "e2f2a8033549421ba51ab5e4e4f0a117",
      "f639ebb541ff4187b3b0d85065689b2d",
      "417bd9f68f254e18945b2835743a624e",
      "12853012e8c142b99fb40e156a5c6d8d",
      "e545b6db6d8743dd895c531914d956a7",
      "f66aa7f1e07146188f7b27d409653380",
      "841b65d4d6c5431abf2ae00b0102324a",
      "f4f2f4bf52164613b5a9ff1a76bdceac",
      "e0fb11d197fc4aadb97aaf8adc7e72ff",
      "91505e7554e64af8ae79ad77614cd7db",
      "c0dd05b8cd22450cb113c268532fadb4",
      "2de2163451c74c9094c3e0b106eee700",
      "8ad3c64f67f1449488d84530c998dab9",
      "4941916358fa48f8a0b7f68baec64e05",
      "b0a19ffdaa7a480591608b25b0ef4f8a",
      "75bd2f1ac6c944a9835b60d35b9e1a1e",
      "4f46aa8647d44fdeaeedb82fa9c6071a",
      "0240f597aa3c4f759ab357df5a5d0383",
      "7ab79aeec7bc47729f51f47bd562b4a5",
      "80599459ddf540fe9caffdf9aed97457",
      "03ce89f979284a8d9cc92c4c5a91acbf",
      "4a5bad9c94644f1f800feecbdb9681bb",
      "4fa90a03b31c4632a7cbe3d0fbb8d35b",
      "0c2f7511f7b04e08850aae15b28ecd80",
      "ab3cf1c21b6d418c85d73e482c0535f0",
      "1bc1215ecb3144459f1fc33356f385e7",
      "8fc005ede53b4b6ba35672ee80ccd713",
      "f5126f41c0e3468cbae6b4d234a1e275",
      "6e29dfbae3a64653911d735aa8b668ed",
      "00e1109d29af4623a10cf951e7d85a1f",
      "2e6ced0acb004865bd4a7f57d372c079",
      "732c870806554b58ae961de69df5d31d",
      "af526e213d0e453799037c9b1277c901",
      "e7ceb58bb82740e8a99bffe08a2510c7",
      "14beb610ae8a43c3b59c2622e790a185",
      "dcf95c4593034506b10217a9dd4dfa1d",
      "887c7ceb8739415886156fa472994878",
      "5af93fa7eaaf41239440c0389fde1752",
      "57b49296dada48b6ba36a1ba57c5c51f",
      "f0e4c78ec40c4162a33ca30f42f51d14",
      "c02adbd171dd40a6bc213e8f9afb1f12",
      "720168020c2d48bc856532a9183d1058",
      "edb58275203348a7b6cc985b6a831422",
      "5db0c9a37c464e63b13d06879d4126d2",
      "7bdb28fc5a3443e18f55059aa2c37a18",
      "85f8e19f23ff4962868c4edfa8a25379",
      "341f370032ed40289362e4ac4f923f4e",
      "adcec62cbb384c2e87fd315f44ccaf3d",
      "c0c9c60f4eb84cf9938eefb3a6cef57e",
      "8f75ebb4cd6e47fcab7348531616a118",
      "902c857f6d0e4ae69d6c1ae43787c960",
      "f2212a08d43341918504924696434b23",
      "09a26bacb22d4c609cbf96bafb84eb0a",
      "bd5a535160474e268b7cf279f2ffe146",
      "9457e6683d354ce6aaecad0080f324ad",
      "032f47b23dac4bfa967f19a8cc81110d",
      "5d5ba6824d8042bc93a05e9c40e65956",
      "3a1a07295114438c8917a1623a285049",
      "8f0068109e804d44ae328a9d16a04e89",
      "96075c1a28f24e1b8667c1decdb938c8",
      "ab3c7cc0b7364868a0555614dd951728",
      "0c3dfc885d304257bd2ef206b9b96778",
      "822ce843b2fc4df991a46ab6d4983312",
      "7f21ea263ed54e409b1e2f173a64e7c2",
      "ef2598fa4c6b45758b613eaee8c4c1d2",
      "f9b8f57ec4bc4adba9d5d250b485c830",
      "997187264d0c496585a1b9d50fd78dbe",
      "3999ab8404fa4d7a9312e05277abd183",
      "0c0564dbab32485e836a0eebab36b6fd",
      "2bdea0601703487c9f1f706caec12daa",
      "336623cdc3884f23b6d35ff0b85b109e",
      "03ddeb15509f47ae89a104d71eea5160",
      "52823a09a4aa453e8d8353b615e29191",
      "9e378d8b5b8c41d6b741b41b0d2c15ef",
      "506abe0b0d36456aaede883c6abab62c",
      "377cf87570484ae0a714b998bdaacc90",
      "ae21061129d344d58921c512c9f9837c",
      "d48d953ae6c94c03b2da22f67902ed06",
      "a15b7a2d214245cbb6dd96d625497180",
      "93fe0046a40e4357974543a160ee4703",
      "192d4ec9a3a64b79ba243aa934401a40",
      "0a46d70847bf46cd94d16092b8e8bcb8",
      "3c2cbae84b8649b094c1877ef8562bf0",
      "5e9e58e4238243f5a43a68ce7fe1e172",
      "e554207781ba44c695c8ebaa4a1df2db",
      "35031a8cfb2942189938766dedd93d5a",
      "38f8e174ee374c298542c0a2cf165a50",
      "25794793375e4aa7aeb23668df776737",
      "5d9873beac6448ca9be91130f625703d",
      "69f2abc5b4594020b78df6e61c2b4436",
      "5cc604049daf495b8fb62eaf67aff745",
      "6b6a412facc54ffbbe9c2760bb9ddcec",
      "919c977e957045d098d2d597306c9112",
      "68fe542c7bdd4d6f8add3715cd5b949a",
      "34cdc616b06543088aa50c04198cf3e4",
      "a51ae88217b44fe38ba343b30177e9bf",
      "0ad2abcc41f14db9938277f6b0d25916",
      "3cd3828c31c948a49901cb8eb07c9848",
      "f99380ed76ee449e9d04fad2c4a7cac1",
      "8ab30c7315f346a6a2fc7facc011e126",
      "3ce0376e43df43f5a1caa13aab8b7269",
      "7236edb2922148d6a901522ae1b0edf6",
      "4f8bd918571f4ae2aaf01bfbb4fafe62",
      "b4955cd037bb4a8980bdeece8044ecd8",
      "7d8d3c0759344f4592d02b3c779b76b3",
      "e934dd66249e4c96b420eb0929d2f42e",
      "4c63697af5e147569829679f259b888d",
      "45489226d74d40038ec0befccbe031e8",
      "25cde9e7fc6b463b9a39af046f5242e8",
      "9f2ecb94b6904bc9993a62cfde3ca68a",
      "5880c779b957437282965e8dff7bbd87",
      "fcfe064c46c94d688226f1c6ed96bf58",
      "87a45b21920e41c6a11deb745699ddad",
      "8052944facb14d2690680c69ea0ca6c1",
      "02ebbba7b93948d29a51ad311245e3ef",
      "e06e84a60ea74c0394f47bd74bab8879",
      "af0e58c3e4fb432894611c52ae5bc85b",
      "966b53ef70b146ad87a8f43cfec50a21"
     ]
    },
    "id": "L6dOebH-rVQe",
    "outputId": "ce4f2d33-ddd5-4791-f395-0966b3cd62d2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841c1311940c4dbdabcf25323b69aff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fb11d197fc4aadb97aaf8adc7e72ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80599459ddf540fe9caffdf9aed97457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6ced0acb004865bd4a7f57d372c079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720168020c2d48bc856532a9183d1058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a26bacb22d4c609cbf96bafb84eb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f21ea263ed54e409b1e2f173a64e7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506abe0b0d36456aaede883c6abab62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35031a8cfb2942189938766dedd93d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad2abcc41f14db9938277f6b0d25916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45489226d74d40038ec0befccbe031e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM meta-llama/Llama-2-7b-chat-hf loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "print(f\"LLM {model_id} loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN1cwk6JtL5b"
   },
   "source": [
    "<h2>Streamlining the process</h2>\n",
    "<h4>The first function takes the PDF and builds the vector database.</h4>\n",
    "<h4>The second function takes the user query, vector database and the model with it's tokenizer to ask the question to the LLM with the prompt given by our prompt function.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "wOMANKvlrWm-"
   },
   "outputs": [],
   "source": [
    "def process_pdf_to_vector_database(pdf_file):\n",
    "    text = read_and_clean_pdf(pdf_file)\n",
    "    chunks = chunk_text(text)\n",
    "    vector_db = embed_and_store_chunks(chunks)\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "jo2VOI8byaBB"
   },
   "outputs": [],
   "source": [
    "def answer_query_with_llm(query, vector_db, llm_model, llm_tokenizer):\n",
    "\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, vector_db, generate_paraphrases)\n",
    "\n",
    "\n",
    "    final_prompt = prompt(query, relevant_chunks)\n",
    "\n",
    "    input_ids = llm_tokenizer.encode(final_prompt, return_tensors=\"pt\").to(llm_model.device)\n",
    "\n",
    "    output = llm_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=10000,\n",
    "        do_sample=False,\n",
    "        eos_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_tokens = output[0][input_ids.shape[-1]:]\n",
    "    answer = llm_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    return answer.strip(), relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDH0jqbmtL5c"
   },
   "source": [
    "<h2>Testing the function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "DPieTjIEycFr"
   },
   "outputs": [],
   "source": [
    "vector_db = process_pdf_to_vector_database('ugrulebook.pdf')  #Sample vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4y99KVCydyi",
    "outputId": "3bbdf181-9c29-4525-b640-334327251889"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The policy for course substitution is as follows:\n",
      "\n",
      "* Course substitution is not permitted for Core Courses, including institute core courses and departmental core courses. Students must re-register and complete these courses.\n",
      "* An Institute elective course may be substituted by another Institute elective course from the same group.\n",
      "* A departmental elective course may be substituted by another departmental elective course from the same group.\n",
      "\n",
      "It is important to note that all students are expected to have 100% attendance in courses. Any student who misses even a single lecture from among the first three lectures of a course is liable to be deregistered from the corresponding course.\n",
      "Time taken = 41.7633101940155\n"
     ]
    }
   ],
   "source": [
    "query='What is the policy for course substitution?'\n",
    "import time\n",
    "start=time.time()\n",
    "answer = answer_query_with_llm(query, vector_db, model, tokenizer)[0]\n",
    "end=time.time()\n",
    "print(answer)\n",
    "print('Time taken =',end-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHITdVW_dGXY"
   },
   "source": [
    "<h2>Building the UI using Gradio</h2>\n",
    "<h4> Github doesn't render interactive UI in preview. A demo video of the UI will be added later.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "mSQ7uyG-ykYH",
    "outputId": "7cd8f871-d532-4ced-8b72-fdd162cc3464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://bf54c8f4c96a01f71e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bf54c8f4c96a01f71e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db = None\n",
    "\n",
    "def handle_pdf_upload(pdf_path):\n",
    "    global vector_db\n",
    "    vector_db = process_pdf_to_vector_database(pdf_path)\n",
    "    return \"PDF processed and ready for your question.\"\n",
    "\n",
    "def handle_question(query):\n",
    "    if not vector_db:\n",
    "        return \"Please upload and process a PDF first.\", \"\"\n",
    "    answer, chunks = answer_query_with_llm(query, vector_db, model, tokenizer)\n",
    "    formatted_chunks = \"\\n\\n\".join([f\"{i+1}. {chunk.strip()}\" for i, chunk in enumerate(chunks) if chunk.strip()])\n",
    "\n",
    "    return answer, formatted_chunks\n",
    "\n",
    "theme = gr.themes.Base(\n",
    "    primary_hue=\"blue\",\n",
    "    secondary_hue=\"gray\",\n",
    "    neutral_hue=\"slate\"\n",
    ").set(\n",
    "    body_text_color=\"#f4f4f4\",\n",
    "    background_fill_primary=\"#1e1e2f\",\n",
    "    input_background_fill=\"#2b2b3c\",\n",
    "    button_primary_background_fill=\"#3b82f6\",\n",
    "    button_primary_text_color=\"#ffffff\",\n",
    "    button_primary_background_fill_hover=\"#2563eb\" # Darker hover\n",
    ")\n",
    "with gr.Blocks(theme = theme, title='PDF Q&A Chatbot') as demo:\n",
    "\n",
    "\n",
    "    with gr.Row(variant=\"panel\"):\n",
    "        with gr.Column(variant=\"panel\"):\n",
    "            pdf_viewer = PDF(label=\"Preview PDF\")\n",
    "        with gr.Column(variant=\"panel\"):\n",
    "            process_btn = gr.Button(\"Process PDF\")\n",
    "            status = gr.Textbox(label=\"Status\", interactive=False, lines = 1)\n",
    "\n",
    "    process_btn.click(\n",
    "        fn=handle_pdf_upload,\n",
    "        inputs=pdf_viewer,\n",
    "        outputs=status\n",
    "    )\n",
    "\n",
    "    with gr.Row(variant=\"panel\"):\n",
    "        question = gr.Textbox(label=\"Ask a question here\")\n",
    "        ask_btn = gr.Button(\"Ask\")\n",
    "\n",
    "    answer = gr.Textbox(label=\"Answer ( This might take some time, the larger your pdf, the longer it takes!)\")\n",
    "    chunks_box = gr.Textbox(label=\"We gave the response to your query from these retrieved chunks of text from the PDF text.\")\n",
    "\n",
    "    ask_btn.click(\n",
    "        fn=handle_question,\n",
    "        inputs=question,\n",
    "        outputs=[answer, chunks_box]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
